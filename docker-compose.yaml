services:

  # This is the first node of the OpenSearch Cluster for storing data
  opensearch-node1:
    image: opensearchproject/opensearch:3
    container_name: opensearch-node1
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g"
      - plugins.security.disabled=true
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=Administrator123!
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data1:/usr/share/opensearch/data
    ports:
      - 9200:9200
      - 9600:9600
    networks:
      - opensearch-net
    healthcheck:
      test: [ "CMD-SHELL", "curl -s -I http://localhost:9200 | grep -q 'HTTP/1.1 200 OK'" ]
      interval: 10s
      timeout: 10s
      retries: 120

  # This is the second node of the OpenSearch Cluster for storing data
  opensearch-node2:
    image: opensearchproject/opensearch:3
    container_name: opensearch-node2
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node2
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g"
      - plugins.security.disabled=true
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=Administrator123!
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data2:/usr/share/opensearch/data
    networks:
      - opensearch-net
    healthcheck:
      test: [ "CMD-SHELL", "curl -s -I http://localhost:9200 | grep -q 'HTTP/1.1 200 OK'" ]
      interval: 10s
      timeout: 10s
      retries: 120

  # This service is used for running OpenSearch Dashboards that interacts with OpenSearch Cluster (node1 and node2)
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:3
    container_name: opensearch-dashboards
    ports:
      - 5601:5601
    expose:
      - "5601"
    environment:
      OPENSEARCH_HOSTS: '["http://opensearch-node1:9200","http://opensearch-node2:9200"]'
      DISABLE_SECURITY_DASHBOARDS_PLUGIN: "true"
    networks:
      - opensearch-net
    depends_on:
      opensearch-node1:
        condition: service_healthy
      opensearch-node2:
        condition: service_healthy

  db_mlflow:
    image: mysql:8-oracle
    restart: always
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 5s
      timeout: 5s
      retries: 5
    environment:
      - MYSQL_DATABASE=mlflow
      - MYSQL_USER=mlflow
      - MYSQL_PASSWORD=mlflow
      - MYSQL_ROOT_PASSWORD=mlflow
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql # Renamed volume to prevent future conflicts

  mlflow_server:
    image: ghcr.io/mlflow/mlflow
    depends_on:
      db_mlflow:
        condition: service_healthy
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=mysql+pymysql://mlflow:mlflow@db_mlflow:3306/mlflow
      - HOSTNAME = ${HOSTNAME:-localhost}
    command: >
      sh -c "pip install PyMySQL[rsa] && mlflow server --backend-store-uri ${DATABASE_URL} --default-artifact-root /mlartifacts --host ${HOSTNAME}"
    volumes:
      - mlflow_artifacts:/mlartifacts

volumes:
  mysql_data:
    driver: local
  mlflow_artifacts:
    driver: local
  opensearch-data1:
  opensearch-data2:

networks:
  opensearch-net:
    driver: bridge
